{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, component\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.12.3',\n",
    "    packages_to_install=[\n",
    "        'pandas',\n",
    "        'mlflow',\n",
    "        'scikit-learn',\n",
    "        'joblib',\n",
    "        'minio',\n",
    "        'azure-storage-blob'\n",
    "    ]\n",
    ")\n",
    "def download_from_azure(\n",
    "    x_train: Output[Dataset],\n",
    "    x_test: Output[Dataset],\n",
    "    y_train: Output[Dataset],\n",
    "    y_test: Output[Dataset],\n",
    "    azure_connection_string: str,\n",
    "    container_name: str = 'csvstorage',\n",
    "    x_train_blob: str=\"X_train.csv\",\n",
    "    x_test_blob: str=\"X_test.csv\",\n",
    "    y_train_blob: str=\"y_train.csv\",\n",
    "    y_test_blob: str=\"y_test.csv\"\n",
    "):\n",
    "    import pandas as pd\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "    # Initialize Azure Blob Service Client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=azure_connection_string)\n",
    "    \n",
    "    # Download function\n",
    "    def download_blob_to_file(blob_name, output_path):\n",
    "        blob_client = blob_service_client.get_blob_client(container='csvstorage', blob=blob_name)\n",
    "        with open(output_path, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    # Download all files\n",
    "    download_blob_to_file(x_train_blob, x_train.path)\n",
    "    download_blob_to_file(x_test_blob, x_test.path)\n",
    "    download_blob_to_file(y_train_blob, y_train.path)\n",
    "    download_blob_to_file(y_test_blob, y_test.path)\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.12.3',\n",
    "    packages_to_install=[\n",
    "        'pandas',\n",
    "        'mlflow',\n",
    "        'scikit-learn',\n",
    "        'joblib',\n",
    "        'minio',\n",
    "        'dagshub',\n",
    "        'prometheus-client'\n",
    "    ]\n",
    ")\n",
    "def train_and_evaluate(\n",
    "    x_train: Input[Dataset],\n",
    "    x_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    best_model: Output[Model],\n",
    "    metrics_port: int = 8000  # Port to expose Prometheus metrics\n",
    "):\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    import joblib\n",
    "    from minio import Minio\n",
    "    import dagshub\n",
    "    from prometheus_client import start_http_server, Gauge\n",
    "    import time\n",
    "    \n",
    "    # Start Prometheus metrics server\n",
    "    start_http_server(metrics_port)\n",
    "    \n",
    "    # Create Prometheus metrics\n",
    "    r2_metric = Gauge('model_r2_score', 'Best model R2 score')\n",
    "    mse_metric = Gauge('model_mse', 'Best model Mean Squared Error')\n",
    "    rmse_metric = Gauge('model_rmse', 'Best model Root Mean Squared Error')\n",
    "    model_type = Gauge('model_type', 'Best model type', ['model_name'])\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Initialize DagsHub and MLflow\n",
    "    dagshub.auth.add_app_token(\"c1b64f0e0a5268dae2ca62d0ae4bec20fdecb445\")\n",
    "    dagshub.init(repo_owner='manish-bagdwal1', repo_name='MLOps-Pipeline-Local-Batch-Training', mlflow=True)\n",
    "\n",
    "    # Load Preprocessed Data\n",
    "    X_train = pd.read_csv(x_train.path)\n",
    "    X_test = pd.read_csv(x_test.path)\n",
    "    y_train = pd.read_csv(y_train.path).values.ravel()\n",
    "    y_test = pd.read_csv(y_test.path).values.ravel()\n",
    "\n",
    "    # Define Models and Hyperparameter Grid\n",
    "    models = {\n",
    "        'LinearRegression': (LinearRegression(), {}),\n",
    "        'Ridge': (Ridge(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'Lasso': (Lasso(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'RandomForest': (RandomForestRegressor(), {'n_estimators': [50, 100, 200]})\n",
    "    }\n",
    "\n",
    "    best_model_instance = None\n",
    "    best_score = float('-inf')\n",
    "    best_model_name = ''\n",
    "    best_mse = float('inf')\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    mlflow.set_tracking_uri('https://dagshub.com/manish-bagdwal1/MLOps-Pipeline-Local-Batch-Training.mlflow')\n",
    "    mlflow.set_experiment('kubeflow_experiment')\n",
    "\n",
    "    # Train and Evaluate Models\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        with mlflow.start_run(run_name=model_name):\n",
    "            if param_grid:\n",
    "                grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                model_instance = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "            else:\n",
    "                model_instance = model.fit(X_train, y_train)\n",
    "                best_params = {}\n",
    "            \n",
    "            predictions = model_instance.predict(X_test)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "            \n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_metric('R2 Score', r2)\n",
    "            mlflow.log_metric('MSE', mse)\n",
    "            mlflow.log_metric('RMSE', rmse)\n",
    "            mlflow.sklearn.log_model(model_instance, model_name)\n",
    "            \n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_mse = mse\n",
    "                best_rmse = rmse\n",
    "                best_model_instance = model_instance\n",
    "                best_model_name = model_name\n",
    "\n",
    "    # Save Best Model\n",
    "    joblib.dump(best_model_instance, best_model.path)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n",
    "    if not client.bucket_exists(\"models\"):\n",
    "        client.make_bucket(\"models\")\n",
    "    client.fput_object('models', f'{best_model_name}.pkl', best_model.path)\n",
    "    \n",
    "    # Update Prometheus metrics\n",
    "    r2_metric.set(best_score)\n",
    "    mse_metric.set(best_mse)\n",
    "    rmse_metric.set(best_rmse)\n",
    "    model_type.labels(model_name=best_model_name).set(1)\n",
    "    \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineTask' object has no attribute 'add_pod_annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129;43m@dsl\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRegression Model Training Pipeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mA pipeline that downloads data from Azure Blob Storage and trains models\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mml_pipeline\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_connection_string\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontainer_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsvstorage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train_blob\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX_train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_test_blob\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX_test.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_blob\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test_blob\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_test.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_port\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8000\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Download data from Azure\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_task\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_from_azure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mazure_connection_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_connection_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontainer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontainer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test_blob\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test_blob\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Train models\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GDS MLOPs\\Project-1\\.venv\\Lib\\site-packages\\kfp\\dsl\\pipeline_context.py:71\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(func, name, description, pipeline_root, display_name, pipeline_config)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[32m     69\u001b[39m     func.pipeline_root = pipeline_root\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GDS MLOPs\\Project-1\\.venv\\Lib\\site-packages\\kfp\\dsl\\component_factory.py:708\u001b[39m, in \u001b[36mcreate_graph_component_from_func\u001b[39m\u001b[34m(func, name, description, display_name, pipeline_config)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[32m    698\u001b[39m \n\u001b[32m    699\u001b[39m \u001b[33;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[33;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    703\u001b[39m component_spec = extract_component_interface(\n\u001b[32m    704\u001b[39m     func,\n\u001b[32m    705\u001b[39m     description=description,\n\u001b[32m    706\u001b[39m     name=name,\n\u001b[32m    707\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Desktop\\GDS MLOPs\\Project-1\\.venv\\Lib\\site-packages\\kfp\\dsl\\graph_component.py:61\u001b[39m, in \u001b[36mGraphComponent.__init__\u001b[39m\u001b[34m(self, component_spec, pipeline_func, display_name, pipeline_config)\u001b[39m\n\u001b[32m     52\u001b[39m     args_list.append(\n\u001b[32m     53\u001b[39m         pipeline_channel.create_pipeline_channel(\n\u001b[32m     54\u001b[39m             name=arg_name,\n\u001b[32m     55\u001b[39m             channel_type=input_spec.type,\n\u001b[32m     56\u001b[39m             is_artifact_list=input_spec.is_artifact_list,\n\u001b[32m     57\u001b[39m         ))\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context.Pipeline(\n\u001b[32m     60\u001b[39m         \u001b[38;5;28mself\u001b[39m.component_spec.name) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     pipeline_outputs = \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline.tasks:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mTask is missing from pipeline.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mml_pipeline\u001b[39m\u001b[34m(azure_connection_string, container_name, x_train_blob, x_test_blob, y_train_blob, y_test_blob, metrics_port)\u001b[39m\n\u001b[32m     25\u001b[39m train_task = train_and_evaluate(\n\u001b[32m     26\u001b[39m     x_train=download_task.outputs[\u001b[33m'\u001b[39m\u001b[33mx_train\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     27\u001b[39m     x_test=download_task.outputs[\u001b[33m'\u001b[39m\u001b[33mx_test\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     28\u001b[39m     y_train=download_task.outputs[\u001b[33m'\u001b[39m\u001b[33my_train\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     29\u001b[39m     y_test=download_task.outputs[\u001b[33m'\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Configure Prometheus scraping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrain_task\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_pod_annotation\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mprometheus.io/scrape\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m train_task.add_pod_annotation(\u001b[33m\"\u001b[39m\u001b[33mprometheus.io/port\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(metrics_port))\n\u001b[32m     35\u001b[39m train_task.add_pod_annotation(\u001b[33m\"\u001b[39m\u001b[33mprometheus.io/path\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/metrics\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'PipelineTask' object has no attribute 'add_pod_annotation'"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Regression Model Training Pipeline',\n",
    "    description='A pipeline that downloads data from Azure Blob Storage and trains models'\n",
    ")\n",
    "def ml_pipeline(\n",
    "    azure_connection_string: str ,\n",
    "    container_name: str = \"csvstorage\",\n",
    "    x_train_blob: str = \"X_train.csv\",\n",
    "    x_test_blob: str = \"X_test.csv\",\n",
    "    y_train_blob: str = \"y_train.csv\",\n",
    "    y_test_blob: str = \"y_test.csv\",\n",
    "    metrics_port: int = 8000\n",
    "):\n",
    "    # Download data from Azure\n",
    "    download_task = download_from_azure(\n",
    "        azure_connection_string=azure_connection_string,\n",
    "        container_name=container_name,\n",
    "        x_train_blob=x_train_blob,\n",
    "        x_test_blob=x_test_blob,\n",
    "        y_train_blob=y_train_blob,\n",
    "        y_test_blob=y_test_blob\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    train_task = train_and_evaluate(\n",
    "        x_train=download_task.outputs['x_train'],\n",
    "        x_test=download_task.outputs['x_test'],\n",
    "        y_train=download_task.outputs['y_train'],\n",
    "        y_test=download_task.outputs['y_test']\n",
    "    )\n",
    "    \n",
    "    # Configure Prometheus scraping\n",
    "    train_task.add_pod_annotation(\"prometheus.io/scrape\", \"true\")\n",
    "    train_task.add_pod_annotation(\"prometheus.io/port\", str(metrics_port))\n",
    "    train_task.add_pod_annotation(\"prometheus.io/path\", \"/metrics\")\n",
    "    \n",
    "    # Optional: Add timeout to ensure metrics are scraped\n",
    "    train_task.set_timeout(180)  # 3 minutes timeout\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(ml_pipeline, 'kubeflow_mlflow_pipeline_promv2.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
