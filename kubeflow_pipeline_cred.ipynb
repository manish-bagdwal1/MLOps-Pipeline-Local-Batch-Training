{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, component\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.12.3',\n",
    "    packages_to_install=[\n",
    "        'pandas',\n",
    "        'mlflow',\n",
    "        'scikit-learn',\n",
    "        'joblib',\n",
    "        'minio',\n",
    "        'azure-storage-blob'\n",
    "    ]\n",
    ")\n",
    "def download_from_azure(\n",
    "    x_train: Output[Dataset],\n",
    "    x_test: Output[Dataset],\n",
    "    y_train: Output[Dataset],\n",
    "    y_test: Output[Dataset],\n",
    "    azure_connection_string: str,\n",
    "    container_name: str = 'csvstorage',\n",
    "    x_train_blob: str=\"X_train.csv\",\n",
    "    x_test_blob: str=\"X_test.csv\",\n",
    "    y_train_blob: str=\"y_train.csv\",\n",
    "    y_test_blob: str=\"y_test.csv\"\n",
    "):\n",
    "    import pandas as pd\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "    # Initialize Azure Blob Service Client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=azure_connection_string)\n",
    "    \n",
    "    # Download function\n",
    "    def download_blob_to_file(blob_name, output_path):\n",
    "        blob_client = blob_service_client.get_blob_client(container='csvstorage', blob=blob_name)\n",
    "        with open(output_path, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    # Download all files\n",
    "    download_blob_to_file(x_train_blob, x_train.path)\n",
    "    download_blob_to_file(x_test_blob, x_test.path)\n",
    "    download_blob_to_file(y_train_blob, y_train.path)\n",
    "    download_blob_to_file(y_test_blob, y_test.path)\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.12.3',\n",
    "    packages_to_install=[\n",
    "        'pandas',\n",
    "        'mlflow',\n",
    "        'scikit-learn',\n",
    "        'joblib',\n",
    "        'minio',\n",
    "        'dagshub'\n",
    "    ]\n",
    ")\n",
    "def train_and_evaluate(\n",
    "    x_train: Input[Dataset],\n",
    "    x_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    best_model: Output[Model]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score\n",
    "    import joblib\n",
    "    from minio import Minio\n",
    "    import dagshub\n",
    "    \n",
    "    dagshub.auth.add_app_token(\"c1b64f0e0a5268dae2ca62d0ae4bec20fdecb445\")\n",
    "    dagshub.init(repo_owner='manish-bagdwal1', repo_name='MLOps-Pipeline-Local-Batch-Training', mlflow=True)\n",
    "\n",
    "    # Load Preprocessed Data\n",
    "    X_train = pd.read_csv(x_train.path)\n",
    "    X_test = pd.read_csv(x_test.path)\n",
    "    y_train = pd.read_csv(y_train.path).values.ravel()\n",
    "    y_test = pd.read_csv(y_test.path).values.ravel()\n",
    "\n",
    "    # Define Models and Hyperparameter Grid\n",
    "    models = {\n",
    "        'LinearRegression': (LinearRegression(), {}),\n",
    "        'Ridge': (Ridge(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'Lasso': (Lasso(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'RandomForest': (RandomForestRegressor(), {'n_estimators': [50, 100, 200]})\n",
    "    }\n",
    "\n",
    "    best_model_instance = None\n",
    "    best_score = float('-inf')\n",
    "    best_model_name = ''\n",
    "\n",
    "    mlflow.set_tracking_uri('https://dagshub.com/manish-bagdwal1/MLOps-Pipeline-Local-Batch-Training.mlflow')\n",
    "    mlflow.set_experiment('kubeflow_experiment')\n",
    "\n",
    "    # Train and Evaluate Models\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        with mlflow.start_run(run_name=model_name):\n",
    "            if param_grid:\n",
    "                grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                model_instance = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "            else:\n",
    "                model_instance = model.fit(X_train, y_train)\n",
    "                best_params = {}\n",
    "            \n",
    "            predictions = model_instance.predict(X_test)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            \n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_metric('R2 Score', r2)\n",
    "            mlflow.sklearn.log_model(model_instance, model_name)\n",
    "            \n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_model_instance = model_instance\n",
    "                best_model_name = model_name\n",
    "\n",
    "    # Save Best Model\n",
    "    joblib.dump(best_model_instance, best_model.path)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    client = Minio('minio-service:9000', access_key='minio', secret_key='minio123', secure=False)\n",
    "    # Create bucket if missing\n",
    "    if not client.bucket_exists(\"models\"):\n",
    "        client.make_bucket(\"models\")\n",
    "    client.fput_object('models', f'{best_model_name}.pkl', best_model.path)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Regression Model Training Pipeline',\n",
    "    description='A pipeline that downloads data from Azure Blob Storage and trains models'\n",
    ")\n",
    "def ml_pipeline(\n",
    "    azure_connection_string: str ,\n",
    "    container_name: str = \"csvstorage\",\n",
    "    x_train_blob: str = \"X_train.csv\",\n",
    "    x_test_blob: str = \"X_test.csv\",\n",
    "    y_train_blob: str = \"y_train.csv\",\n",
    "    y_test_blob: str = \"y_test.csv\"\n",
    "):\n",
    "    # Download data from Azure\n",
    "    download_task = download_from_azure(\n",
    "        azure_connection_string=azure_connection_string,\n",
    "        container_name=container_name,\n",
    "        x_train_blob=x_train_blob,\n",
    "        x_test_blob=x_test_blob,\n",
    "        y_train_blob=y_train_blob,\n",
    "        y_test_blob=y_test_blob\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    train_task = train_and_evaluate(\n",
    "        x_train=download_task.outputs['x_train'],\n",
    "        x_test=download_task.outputs['x_test'],\n",
    "        y_train=download_task.outputs['y_train'],\n",
    "        y_test=download_task.outputs['y_test']\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(ml_pipeline, 'kubeflow_mlflow_pipeline_v14.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
